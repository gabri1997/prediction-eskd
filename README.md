# ANN Training for ESKD Prediction

This repository contains a PyTorch implementation of a **binary classification neural network** to predict the occurrence of End-Stage Kidney Disease (ESKD) using clinical variables. The network is trained using **stratified 10-fold cross-validation** on 80% of the data and validated/tested on the remaining 20%. The training process is integrated with **Weights & Biases (wandb)** for experiment tracking.

In the paper, the authors use **KFold** cross-validation, but in this work I use **StratifiedShuffleSplit (SSS)** instead. The main difference between **KFold** and **StratifiedKFold** lies in how class distributions are handled across folds. In **KFold**, the dataset is split into (k) folds of roughly equal size without considering class proportions, so some folds—especially in imbalanced datasets—may overrepresent certain classes. **StratifiedKFold**, on the other hand, ensures that each fold approximately preserves the original class distribution, making train and test sets more representative and providing more reliable evaluation, particularly for imbalanced datasets.

### Data Distribution and Training Strategy

The dataset was split into **80% training** and **20% testing** using a stratified shuffle split, ensuring that the class distribution is preserved across both sets.  
With a fixed random seed (`random_state=42`), the resulting distribution is:

- **Training set**  
  - Class 0 → 586 samples (~77.5%)  
  - Class 1 → 170 samples (~22.5%)  

- **Test set**  
  - Class 0 → 146 samples (~77.2%)  
  - Class 1 → 43 samples (~22.8%)  

To prevent overfitting, **early stopping** was applied during training. The decision to stop training is based on the **validation loss**, which serves as the monitoring metric for model generalization performance.


---

## Features

- Handles missing or infinite values in the dataset.
- Uses a simple feed-forward neural network with 4 hidden layers of 100 units each.
- Dropout and batch normalization for regularization.
- Exponential learning rate scheduler.
- Binary cross-entropy loss with class imbalance handling.
- Automatic saving of the best model based on **F1 score**.
- Stratified splitting ensures class balance between training and test sets.
- Logging of training metrics (accuracy, precision, recall, F1 score, loss, learning rate) with **wandb**.

---

## Dataset

The model expects a **Pandas DataFrame** with the following columns:

- `Eskd` → binary target (1 if ESKD occurred, 0 otherwise)
- `Code` → patient identifier (not used in training)
- `Gender` → 'M' or 'F' (automatically encoded as 0/1)
- Remaining columns → clinical features (numerical)

**Notes:**
- Only numeric features are used for training.
- Missing and infinite values are automatically replaced with 0.

---

## Neural Network Architecture

- Input layer → size equals number of features
- 4 hidden layers:
  - 100 neurons each
  - ELU activation
  - BatchNorm
  - Dropout (configurable)
- Output layer → 1 neuron (logit for binary classification)
- Loss → `BCEWithLogitsLoss` with `pos_weight` to handle class imbalance

---

### Hyperparameter Sweeps with Weights & Biases

To optimize a model's performance, we often need to find the best combination of hyperparameters. Weights & Biases (wandb) provides a feature called **Sweep** that automates this search. 

A sweep is defined through a configuration file specifying the hyperparameters to explore, their ranges, and the metric to optimize (e.g., F1 score). The two main commands for running a sweep are:

- `wandb sweep`: Initializes the sweep using the configuration file.
- `wandb agent`: Executes the training runs using the different hyperparameter combinations generated by the sweep.

This allows efficient and systematic exploration of the hyperparameter space without manual tuning.