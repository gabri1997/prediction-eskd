# ANN Training for ESKD Prediction

This repository contains a PyTorch implementation of a **binary classification neural network** to predict the occurrence of End-Stage Kidney Disease (ESKD) using clinical variables. The network is trained using **stratified 10-fold cross-validation** on 80% of the data and validated/tested on the remaining 20%. The training process is integrated with **Weights & Biases (wandb)** for experiment tracking.

In the paper, the authors use **KFold** cross-validation, but in this work I use **StratifiedShuffleSplit (SSS)** instead. The main difference between **KFold** and **StratifiedKFold** lies in how class distributions are handled across folds. In **KFold**, the dataset is split into (k) folds of roughly equal size without considering class proportions, so some folds—especially in imbalanced datasets—may overrepresent certain classes. **StratifiedKFold**, on the other hand, ensures that each fold approximately preserves the original class distribution, making train and test sets more representative and providing more reliable evaluation, particularly for imbalanced datasets.

## Pytorch implementation

### Data Distribution and Training Strategy

The dataset was split into **80% training** and **20% testing** using a stratified shuffle split, ensuring that the class distribution is preserved across both sets.  
With a fixed random seed (`random_state=42`), the resulting distribution is:

- **Training set**  
  - Class 0 → 586 samples (~77.5%)  
  - Class 1 → 170 samples (~22.5%)  

- **Test set**  
  - Class 0 → 146 samples (~77.2%)  
  - Class 1 → 43 samples (~22.8%)  

To prevent overfitting, **early stopping** was applied during training. The decision to stop training is based on the **validation loss**, which serves as the monitoring metric for model generalization performance.

### Batch Balancing and Proxy AUC Loss

To address the class imbalance present in the dataset, a **WeightedRandomSampler** was used during training.  
This sampling strategy assigns higher selection probabilities to samples from the minority class, ensuring that each batch contains a roughly equal number of positive and negative examples.  
By balancing batches at training time, the model avoids bias toward the majority class and learns more discriminative features for both categories.

Instead of the standard **binary cross-entropy (BCE)** loss, a **Proxy AUC loss** was employed.  
The conventional AUC metric is not directly differentiable, as it depends on the ranking of predictions rather than on continuous probability values.  
The Proxy AUC loss provides a differentiable approximation of the AUC by comparing all positive–negative prediction pairs using a sigmoid-based formulation.  
This enables gradient-based optimization of the AUC objective, leading to models that are more robust in imbalanced classification settings where accuracy or BCE-based optimization may be misleading.

---

## Features

- Handles missing or infinite values in the dataset.
- Uses a simple feed-forward neural network with 4 hidden layers of 100 units each.
- Dropout and batch normalization for regularization.
- Exponential learning rate scheduler.
- Binary cross-entropy loss with class imbalance handling.
- Automatic saving of the best model based on **F1 score**.
- Stratified splitting ensures class balance between training and test sets.
- Logging of training metrics (accuracy, precision, recall, F1 score, loss, learning rate) with **wandb**.

---

## Dataset

The model expects a **Pandas DataFrame** with the following columns:

- `Eskd` → binary target (1 if ESKD occurred, 0 otherwise)
- `Code` → patient identifier (not used in training)
- `Gender` → 'M' or 'F' (automatically encoded as 0/1)
- Remaining columns → clinical features (numerical)

**Notes:**
- Only numeric features are used for training.
- Missing and infinite values are automatically replaced with 0.

---

## Neural Network Architecture

- Input layer → size equals number of features
- 4 hidden layers:
  - 100 neurons each
  - ELU activation
  - BatchNorm
  - Dropout (configurable)
- Output layer → 1 neuron (logit for binary classification)
- Loss → `BCEWithLogitsLoss` with `pos_weight` to handle class imbalance

---

### Hyperparameter Sweeps with Weights & Biases

To optimize a model's performance, we often need to find the best combination of hyperparameters. Weights & Biases (wandb) provides a feature called **Sweep** that automates this search. 

A sweep is defined through a configuration file specifying the hyperparameters to explore, their ranges, and the metric to optimize (e.g., F1 score). The two main commands for running a sweep are:

- `wandb sweep`: Initializes the sweep using the configuration file.
- `wandb agent`: Executes the training runs using the different hyperparameter combinations generated by the sweep.

This allows efficient and systematic exploration of the hyperparameter space without manual tuning.

## TensorFlow / Keras Implementation

A full **TensorFlow/Keras** version of the model is also available in this repository.  
It replicates the architecture and training logic of the PyTorch implementation, including:

- Four hidden layers (100 neurons each)  
- ELU activation and Batch Normalization after each layer  
- Dropout for regularization  
- **Proxy AUC Loss** function for differentiable AUC optimization  
- Custom early stopping inspired by *Prechelt (1998)*  
- Evaluation on 5-year and 10-year follow-up subsets  

### Files involved

The Keras implementation is split across three scripts for clarity:

- `train_eval.py` → main training and evaluation script (to be executed)
- `processing.py` → data preprocessing and feature engineering functions
- `model.py` → model definition and custom loss functions

### How to run

To train and evaluate the TensorFlow/Keras model, simply execute:

```bash
python train_eval.py

To train and evaluate the Pytorch model, you have to execute:

```bash
net_sweep_train_80_20_split.py
net_sweep_eval_80_20_split.py
